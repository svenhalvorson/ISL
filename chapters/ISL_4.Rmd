---
title: 'Chapter 4: Classification'
subtitle: "<span style='font-size: 20px'>Notes from ISL</style>"
author: 'Sven Halvorson'
date: "7/9/2020"
output:
  html_document:
    toc: yes
    toc_depth: 4
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library('tidyverse')
```

### Definitions
<u><span style='font-size: 18px'>4.2</u></style>
* **Classification problem/classifier**: When the response variable is categorical, a classifier is our estimate of $f$.
<u><span style='font-size: 18px'>4.2</u></style>
* **Logistic model** Classification strategy where we model the probability of $Y=1$ with the logistic function.
* **Logit**: Logarithm of an odds
* **Maximum likelihood estimation**: A method of selecting parameters wherin we model the parameters as a function of the training data and then pick the parameter estimates based on maximizing the likelihood function.
<u><span style='font-size: 18px'>4.2</u></style>
* **Prior**: Assumed probabilities made before modeling. Sometimes come from the data themselves, other data sources, or other studies.
* **Posterior probability**: Modification made to the prior using the data.

### Notes

#### 4.2: Why not linear regression?

With lots of classification problems, the reason why linear regression is not appropriate is that the coding schemes implicitly put distances between the different classes when there may be none at all. The example of mild, medium, and severe are ordered but it's not clear that the distance between mild and medium is the same as medium to severe. The case of binary variables is closer to appropriate but linear regression will fail here in that it will sometimes predict values larger than 1 or less than zero.

The book uses this shorthand $P(Y=1|X)=p(x)$ which... I don't like that much but we'll stick with it.

#### 4.3: Logistic regression
 
In order to deal with the issue of potentially predicting values outside of (0,1), we can instead use a transformation (link) of the probability of X. The most commonly used one is the *logistic function*:

$$p(X) = \frac{e^{\beta_0+\beta_1X}}{1+e^{\beta_0+\beta_1X}}$$
Here's a graph of the logistic function with different values of $\beta_0=0$ and $\beta_1=1$:

```{r logistic_graph, echo = FALSE}

logistic = function(x, b0, b1){
  exp(b0+x*b1)/(1+exp(b0+x*b1))
}


lab_tab = tibble(
  model = c(
    'Y1', 'Y2', 'Y3', 'Y4'
  ),
  lab = factor(
    c(
      'B0 = 0, B1 = 1',
      'B0 = 10, B1 = 1',
      'B0 = 0, B1 = -4',
      'B0 = 0, B1 = 0.25'
    ),
    levels = c(
      'B0 = 0, B1 = 1',
      'B0 = 10, B1 = 1',
      'B0 = 0, B1 = -4',
      'B0 = 0, B1 = 0.25'
    )
  )
)

tibble(
  X = seq(from = -25, to = 25, by = 0.1),
  Y1 = logistic(X, 0, 1),
  Y2 = logistic(X, 10, 1),
  Y3 = logistic(X, 0, -4),
  Y4 = logistic(X, 0, 0.25)
) %>% 
  pivot_longer(
    cols = matches('Y'),
    names_to = 'model',
    values_to = 'Y'
  ) %>% 
  left_join(lab_tab) %>% 
  ggplot(aes(x = X, y = Y)) +
  geom_point(size = 1.5, color = 'firebrick') +
  theme_classic() +
  labs(
    x = 'X',
    y = 'logit(X)'
  ) +
  facet_wrap(~lab)




```

From this, we can see that changes in the probability with respect to the logistic function respond to the coefficients. $\beta_0$ controls where the transition of probability is centered. The sign of $\beta_1$ controls whether low values of $X$ are associated with high or low probabilities of $Y=1$. The magnitude of $\beta1$ controls how quickly the transition happens.

For interpretation, we generally like to reformulate the logistic function like this:

$$\frac{p(X)}{1-p(X)} = e^{\beta_0+\beta_1}$$
By doing this, we can interpret the various betas as either odds, odds ratios, or changes in odds ratio for a given 1 unit change in a continuous predictor. A further transformation of this gets us to:

$$log(\frac{p(X)}{1-p(X)}) = \beta_0+\beta_1$$
Which the left hand side is called the *logit* and logistic regression models this as a linear combination of predictors.

The method of estimating the coefficients is done through *maximum likelihood* which is one of the few statistical concepts that I found to have interesting philosophical parallels. Essentially what we do is select the set of estimates that produces the highest probability of observing the given sample. To me this seems like 'whatever you see, that should not be viewed as an outlier without additional information.' Selecting the coefficients comes from the *likelihood function* which for a binary outcome and two coefficients would be:

$$l(\beta_0,\beta_1) = \sum_{i:y_i=1}p(x_i)\sum_{j:y_j=0}(1-p(x_j))$$
In this case, we're thinking of the data as fixed and we write a function that has the parameters as the only variables. Whatever the global maximum is for $l$, this is which parameters we will choose. In the context of logistic regression it would look like this:

$$l(\beta_0,\beta_1) = \sum_{i:y_i=1}\frac{e^{\beta_0+\beta_1x_i}}{1+e^{\beta_0+\beta_1x_i}}\sum_{j:y_j=0}(1-\frac{e^{\beta_0+\beta_1x_j}}{1+e^{\beta_0+\beta_1x_j}})$$
And then we would probably end up taking a log and turning this more into a polynomial in order to solve it.

The authors then go into some discussion about how predictions are made, extensions to more than one predictor, and confounders that I will not repeat here. They also note that it is possible to extend logistic regression to more than two classes but that it is generally not very effective compared to other methods.

####4.4 Linear discriminant analysis

An alternative to logistic regression is *linear discriminant analysis*. This method is new to me so I'm excited to get into more new material. In a broader sense, the method works somewhat in reverse of logistic regression. The reasons given that this method could be preferable to logistic regression are:
  
* In the case of highly separated levels of $Y$, logistic regression can have unstable parameter estimates. I take this to mean that the standard error of the parameter estimates will be larger in the case that $X$ is highly predictive.
* If n is small and the predictors are approximately normal, then LDA is more more stable. 
* When we have more than one class in the response, LDA is preferable. 
  
Logistic regression works by modeling the transformed response in terms of a linear combination of the predictors. LDA, by contrast, we model the distribution of each predictor in each level of the response . We then use Bayes theorem to reverse direction and model the response in terms of these distributions. To me, this method feels like:

1. Figure out how the distributions of $X$ are within each response class.
2. Determine which distribution the observation most likely came from and select the appropriate class based off this.

Let's get into the details. First we start with a response, $Y$, that can take one of $K\geq2$ classes. For each class, $K=k$, we assume that $\pi_k$ is the unconditional probability $P(Y=k)$. Frequently we would just use the observed frequency within the training data but if we had something like census information that could better inform our priors, that could be used. Then for each class, we define the density function as:

$$f_k(x)=P(X=x|Y=k)$$
So this is the portion where we model the distribution of the predictors for each class. They mention that this is sort of symbolic in this case because the density function for a continuous variable (which is what is described for LDA here) would require a small region around $x$ and not equality. Then we use Bayes theorem to convert this back into a probability of $Y$ given $X$:

$$p_k(x)=P(Y=k|X=x)=\frac{P(Y=k \cap X=x)}{P(X=x)}=\frac{P(Y=k)P(X=x|Y=k)}{\sum_{l=1}^KP(X=x|Y=k)}=\frac{\pi_k\cdot f_k(x)}{\sum_{l=1}^K\pi_lf_l(x)}$$
So then once we have all our estimates of the *priors* ($\pi_k$) and the *posteriors* ($f_k(x)$), we can then compute the probabilities of each training observation being in each outcome class and select the most likely one. The prior probabilities are relatively easy and from my understanding, most Bayesian methods are relatively insensitive to poor choices of prior. The issue is more that computing the posteriors is hard without some assumptions. One method of doing this (I assume there you could use alternative distributions/assumptions depending on how the training data actually looks) is to just assume that you have a normal distribution for the predictor. In this case, we can use the density function (pdf) for the Gaussian distribution for $f_k(x)$:

$$f_k(x)=\frac{1}{\sqrt{2\pi}\sigma_k}\cdot\text{exp}(-\frac{1}{2\sigma_k^2}(x-\mu_k)^2)$$
So this wildboy basically tells us what the probability is of having a predictor value within a narrow interval around $x$. The authors suggest we assume the variance is the same for each response level for the moment. We can substitute the normal pdf in the formula for $p_k(x)$:

$$p_k(x)=\frac{\pi_k\cdot \frac{1}{\sqrt{2\pi\sigma_k}}\cdot\text{exp}(-\frac{1}{2\sigma_k^2}(x-\mu_k)^2)}{\sum_{l=1}^K\pi_l\frac{1}{\sqrt{2\pi\sigma}}\cdot\text{exp}(-\frac{1}{2\sigma^2}(x-\mu_l)^2)}$$
Then, for ease of computation and simplification, they suggest taking a log of this. Because logarithms are increasing, maximizing this will produce the same solution:

$$\delta_k(x)=log(\pi_k\cdot \frac{1}{\sqrt{2\pi}\sigma}\cdot\text{exp}(-\frac{1}{2\sigma^2}(x-\mu_k)^2))-log(\sum_{l=1}^K\pi_l\frac{1}{\sqrt{2\pi}\sigma}\cdot\text{exp}(-\frac{1}{2\sigma^2}(x-\mu_l)^2))$$
So then we can cancel the 

### Lab

```{r silent, echo = FALSE}

```

```{r show_code, echo=FALSE}

```

### Exercises

