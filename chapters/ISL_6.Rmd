---
title: 'Chapter 6: Linear Model Selection & Regularization'
subtitle: "<span style='font-size: 20px'>Notes from ISL</style>"
author: 'Sven Halvorson'
output:
  html_document:
    toc: yes
    toc_depth: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE)
```

### Definitions
<u><span style='font-size: 18px'>6.1</u></style>  
* **Best subset selection**: Choosing the set of predictors that make the 'best model' by some criterion.
* **Mallow's $C-p$**: An unbiased estimate of the test MSE given as $C_p = \frac{1}{n}(RSS+2d\hat{\sigma}^2)$.
* **Akaike's information criterion**: A measure of the information lost for an estimate of $f$. Given by $AIC=2d-2ln(L)$
* **Bayesian information criterion**: Alternative method of model selection given as $BIC=k\cdot ln(n)-2ln(L)$.

<u><span style='font-size: 18px'>6.2</u></style>  
* ** **:
<u><span style='font-size: 18px'>6.3</u></style>  
* ** **:
<u><span style='font-size: 18px'>6.4</u></style>  
* ** **:


  
### Notes

This chapter focuses on a number of tweaks we can make to linear models. The most important concerns addressed here are:  
i. Reducing the variance of models produced in situations where the number of predictors is close to the number of observations.  
ii. Improving interpretability by reducing the total number of predictors.
  
The methods described are:
i. **Subset selection**: Choosing only a subset of predictors to use.
ii. **Shrinkage methods**: Reducing variance of model fitting by diminishing the effect of some or all of the predictors.
iii. **Dimension reduction**:: Reframing the problem in terms of combinations of variables thus reducing the number of used predictors.
  
#### 6.1: Subset Selection

It's often the case that we have many more variables than necessary to create an effective statistical learning model. This can happen because some predictors are unrelated to the outcomes, because a complicated model is more difficult to interpret, or because the number of predictors strains or prohibits the model fit. In order to avoid these problems, we will use *subset selection* methods.  
As a theoretical starting point, we can consider the *best subset selection method*. In some cases, this is possible but I get the feeling that this serves mostly as a foil for the other methods discussed. Essentially, it involves us fitting every ($2^p$) model possible and then using some kind of cross validation method to compare them. The model with the best fit under that method would be chosen ad the 'best subset' of predictors. Unfortunately, this takes a lot of time and will be computationally expensive so it is not ideal for most situations.  
A variation on this is given where we reduce the time spent cross validating by these steps:  
1. Begin with a null model $\mathcal{M}_0$ using no predictors.
2. For $0<i\leq p$, create all the models with $i$ predictors. Select the model with the least RSS as $\mathcal{M}_i$ (or another measure such as deviance)
3. Cross validate each $\mathcal{M}_i$ and select the model with the lowest test MSE, $C_p$, AIC, BIC, adjusted $R^2$, or any other metric.  
Personally, I don't find this algorithm to be that compelling because the only thing that it reduces over testing every possible model is the cross validation step. In practice, the model fitting will probably be the longest time investment and this still involves fitting every possible model.
  
The next set of methods all fall under the heading of *stepwise selection*. These methods involve some kind of iterative re-fitting of models in which we start with some initial model and then repeatedly add and/or remove predictors to the model according to some criteria related to the potential change in model fit. Forward and backward selection work like this:  
1. Begin with a null model $\mathcal{M}_0$. In the case of forward, this is the intercept only. For backward, we start with the full model.
2. Add or remove (depending on direction) predictors one by one and compute a summary statistic for each such as RSS. Select the model the best of these models as $\mathcal{M}_1$
3. For each subsequent $1<i\leq p$, build upon the the previous model. Again consider all of the $p-i+1$ remaining predictors and choose one to add/remove from the model and set the best of these as $\mathcal{M}_i$.
4. Cross validate $\mathcal{M}_0, \mathcal{M}_1, ..., \mathcal{M}_p$ or use another metric to choose the best subset of predictors.  
These are pretty reasonable ways to select a model in my opionin as they do not have to fit nearly as many models ($\frac{p(p+1)}{2}$.  
Some other things to note about stepwise selection methods:
a. Forward and backward selection do not necessarily yield the same results althoug the subsets are often similar or identical.
b. There is no guarantee that the the best model is actually considered in these methods. The are what you might call 'greedy' algorithms in which they try to maximize at each step. The true best model could involve just two predictors, neither of which would be selected in the first round of forward selection.
c. When $n<p$, backwards selection cannot be performedand forward selection will terminate at subsets of size $n$.
d. There are hybrid approaches in which we set criteria to enter as well as leave. These methods sometimes behave like 'after each additional variable is added, the other predictors are then re-evaluated for removal' and then there is some kind of stopping procedure to make sure this doesn't go in loops.
  
  The next portion of the text describes some ways of choosing the 'best' subset. One method, which we've already covered, is just to use the cross validation methods described in chapter 5. Alternatively, there are some methods of adjusting the training error rate in order to select a method that   
*should* have a small testing error rate. The listed methods are Mallow's $C_p$, Akaike information criterion (AIC), Bayesian information criteria (BIC), and the adjusted $R^2$. I think I'll try and do a little more research on these than is listed in the book since the explanations I got for a few of these were pretty handwavy in graduate school.  
  
Mallow's $C_p$ is just a transformation of the residual sum of squares:
$$C_p = \frac{1}{n}(RSS+2d\hat{\sigma}^2)$$


Where $d$ is the number of predictors used and $\hat{\sigma}^2$ is the estimate of the variance of $\epsilon$ from the regression estimate. In the linear regression case I believe this should be the MSE. We're told that if the regression model is unbiased (truly meets assumptions of linearity ect.), then $C_p$ is an unbiased estimate of the testing MSE.  Therefore, we would like to select the model with the smallest value of $C_p$. From the formula, we can also see that $C_p$ puts a penalty on the number of predictors so unlike $R^2$, simpler models can be selected via this method.
An alternative formulation is given as:

$$C'_p=\frac{RSS}{\hat{\sigma}^2}+2d-n$$
Which will have the same minimum value for all the models in consideration.
  
<!-- To dive a bit deeper into this, I looked at [this source](https://online.stat.psu.edu/stat462/node/197/) from the famous PSU courses. They start by starting with the bias variance trade-off saying that the variance in predicted responses are due to: -->
<!-- 1. Sampling variation ($\sigma_{\hat{y}_i}^2$) -->
<!-- 2. Variance due to bias ($B_i^2$) -->

<!-- Summing these two quantities over the entire data and standardizing the results set is: -->

<!-- $$\Gamma_p=\frac{1}{\sigma^2}\left\{\sum_{i=1}^n\sigma_{\hat{y_i}}^2+\sum_{i=1}^n[E(\hat{y_i}-E(y_i)]^2 \right\}$$ -->
<!-- So from this, we can see that the variance portion of this is just the sum of sigma squared which are the variances in response at each point. The variance due to bias is the sum of square differences in predicted values and expectations. -->

The next measure that we could consider here is the *Akaike information criteria* which is given as:

$$AIC=2d-2ln(L)\approx\frac{1}{n\hat{\sigma}^2}(RSS-2d\hat{\sigma}^2)$$
Where $L$ is the likelihood of the model and $-2ln(L)$ is the deviance. So having more predictors increases the AIC as does a smaller $L$ (and thus larger deviance). I'm reading from some other sources that this quantity estimates the 'information lost' via the estimate of $f$. It's not clear what this means exactly this means but it's not something we want and thus we choose to minimize the AIC. There are also corrected versions of this because apparently the desired properties are only asymtotically achieved. Another point to note is that this doesn't tell us anything about the absolute fit fo the model, only the relative efficacy between models.

It's directly proportional to $C_p$ so from that perspective, it probably is not worthwhile computing them both... as far as I can tell.

Another measure in this line is the *Bayesian information criteria* which is quite similar to the AIC but it puts a different weight on the number of predictors:

$$BIC=k\cdot ln(n)-2ln(L)\approx \frac{1}{n\hat{\sigma}^2}(RSS-log(n)d\hat{\sigma}^2)$$
In general, we will have data sets with more than 7 observations so $log(n)>2$. This indicates that the BIC more heavily penalizes models with many predictors than the AIC does. 
  
The last measure listed here is the *adjusted R^2$* which I have heard lots of complaints about but have not looked into the reasoning for these complaints yet. It's given as:
$$\text{Adj. }R^2=1-\frac{RSS/(n-d-1)}{TSS/(n-1)}$$
Here the goal is to maximize the adjusted R$^2$ which really just means minimizing $RSS/(n-d-1)$ since the TSS is fixed for a given data set. Unlike $R^2$, this can potentially go down with more predictors if the addition of a variable does not provide a sufficient reduction in RSS. The authors note that while there is a decent intuition behind the adjusted R$^2$, there is no real theorhetical derivation of this as a useful metric.


#### 6.2: Shrinkage Methods

#### 6.3: Dimension Reduction Methods

#### 6.4: Considerations in High Dimensions

### Lab

```{r silent, echo = FALSE}

```

```{r show_code, echo=FALSE}

```

### Exercises
  
#### 1